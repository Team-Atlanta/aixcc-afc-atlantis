[program:server]
command=uv run vllm serve models/%(ENV_MODEL)s --port %(ENV_SERVER_PORT)s --trust-remote-code --enable-lora --enable-chunked-prefill --tensor-parallel-size 2 --pipeline-parallel-size 1 --enforce-eager --middleware openai_compatible_server.tracing.middlewares.with_opentelemetry --dtype float16
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr
stdout_maxbytes=0
stderr_maxbytes=0
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
environment=CUDA_VISIBLE_DEVICES="0,1",VLLM_ALLOW_RUNTIME_LORA_UPDATING="True",TRANSFORMERS_OFFLINE="1"

[program:adapter]
command=uv run uvicorn --host 0.0.0.0 --port %(ENV_ADAPTER_PORT)s --workers 1 main:app
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr
stdout_maxbytes=0
stderr_maxbytes=0
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
environment=CUDA_VISIBLE_DEVICES="2,3"

[supervisord]
nodaemon=true
