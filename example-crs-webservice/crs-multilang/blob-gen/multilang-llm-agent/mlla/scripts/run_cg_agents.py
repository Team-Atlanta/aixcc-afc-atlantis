import argparse
import asyncio
import json
import os
import tempfile
from pathlib import Path

import tokencost
from dotenv import load_dotenv
from loguru import logger

from ..agents.cpua import deserialize_cpua
from ..agents.orchestrator_agent.modules.call_blobgen_agent import (
    process_blobgen_from_cg,
)
from ..agents.orchestrator_agent.modules.call_generator_agent import (
    process_generator_from_cg,
)
from ..utils.context import GlobalContext

# Load CI environment configuration
load_dotenv("env.ci")
load_dotenv(".env.secret")


try:
    json_path = (
        Path(os.path.dirname(__file__))
        / "../assets"
        / "model_prices_and_context_window.json"
    )
    if json_path.exists():
        data = json.load(open(json_path))
        tokencost.TOKEN_COSTS.update(data)
except Exception as e:
    logger.warning(f"Error updating token costs: {e}")


def load_cgs_file(cgs_file_path):
    """Load CGs from a JSON file."""
    try:
        with open(cgs_file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # Use the deserialize_cpua function to parse the CGs
        cpua_output = deserialize_cpua(content)
        return cpua_output["CGs"]
    except Exception as e:
        logger.error(f"Error loading CGs from {cgs_file_path}: {e}")
        return None


async def run_generator_agent(gc, cg, dst_func, sanitizer, trial_num, total_trials):
    """Run generator agent for a single CG."""
    logger.info(
        f"Trial {trial_num}/{total_trials}: Running GeneratorAgent for CG {cg.name}"
    )

    try:
        result = await process_generator_from_cg(gc, cg, dst_func, sanitizer)

        if result:
            logger.info(f"GeneratorAgent found {len(result)} crashes for CG {cg.name}")
            return result
        else:
            logger.info(f"No crashes found by GeneratorAgent for CG {cg.name}")
            return {}
    except Exception as e:
        logger.error(f"Error running GeneratorAgent for CG {cg.name}: {e}")
        return {}


async def run_blobgen_agent(gc, cg, dst_func, sanitizer, trial_num, total_trials):
    """Run blobgen agent for a single CG."""
    logger.info(
        f"Trial {trial_num}/{total_trials}: Running BlobGenAgent for CG {cg.name}"
    )

    try:
        result = await process_blobgen_from_cg(gc, cg, dst_func, sanitizer)

        if result:
            logger.info(
                f"BlobGenAgent generated {len(result)} payloads for CG {cg.name}"
            )
            return result
        else:
            logger.info(f"No payloads generated by BlobGenAgent for CG {cg.name}")
            return {}
    except Exception as e:
        logger.error(f"Error running BlobGenAgent for CG {cg.name}: {e}")
        return {}


async def main():
    parser = argparse.ArgumentParser(
        description="Run GeneratorAgent and BlobGenAgent on CGs from file"
    )
    parser.add_argument("--cg-path", required=True, help="Path to CGs JSON file")
    parser.add_argument("--cp", default="/src", help="Path to challenge program")
    parser.add_argument(
        "--agent",
        choices=["generator", "blobgen", "both"],
        default="both",
        help="Which agent to run",
    )
    parser.add_argument("--model", default="gpt-4", help="Model to use")
    parser.add_argument("--trials", type=int, default=1, help="Number of trials per CG")
    parser.add_argument(
        "--sanitizer", default="", help="Sanitizer to use (auto-detect if empty)"
    )
    parser.add_argument("--harness", default="", help="Target harness name")
    parser.add_argument(
        "--workdir", default=tempfile.mkdtemp(), help="Working directory"
    )
    parser.add_argument(
        "--output",
        default=tempfile.mkdtemp(),
        help="Output directory for generated blobs",
    )

    args = parser.parse_args()

    # Check if CGs file exists
    cgs_path = Path(args.cg_path)
    if not cgs_path.exists():
        logger.error(f"CGs file {args.cg_path} does not exist")
        return

    # Check if CP path exists
    cp_path = Path(args.cp)
    if not cp_path.exists():
        logger.error(f"Challenge program path {args.cp} does not exist")
        return

    # Load CGs
    cgs_dict = load_cgs_file(args.cg_path)
    if cgs_dict is None:
        return

    logger.info(f"Loaded CGs from {args.cg_path}")
    for harness_name, cgs in cgs_dict.items():
        logger.info(f"Harness {harness_name}: {len(cgs)} CGs")

    # Initialize GlobalContext
    logger.info("Initializing GlobalContext...")
    gc = GlobalContext(
        no_llm=False,
        cp_path=cp_path,
        target_harness=args.harness,
        workdir=args.workdir,
        output_dir=args.output,
        load_agent_names=["cpua", "bcda"],
    )

    async with gc.init():
        logger.info("GlobalContext initialized successfully")

        # Process each harness and CG
        all_generator_results = {}
        all_blobgen_results = {}

        for harness_name, cgs in cgs_dict.items():
            logger.info(f"Processing harness: {harness_name}")

            for cg in cgs:
                logger.info(f"Processing CG: {cg.name}")

                # Find a vulnerable function to target
                dst_func = None

                def find_vulnerable_func(func_info):
                    """Recursively find a vulnerable function."""
                    if (
                        func_info.sink_detector_report
                        and func_info.sink_detector_report.is_vulnerable
                    ):
                        return func_info

                    for child in func_info.children:
                        result = find_vulnerable_func(child)
                        if result:
                            return result
                    return None

                dst_func = find_vulnerable_func(cg.root_node)

                if not dst_func:
                    logger.warning(
                        f"No vulnerable function found in CG {cg.name}, skipping"
                    )
                    continue

                logger.info(f"Targeting function: {dst_func.func_location.func_name}")

                # Run trials for this CG
                for trial in range(args.trials):
                    trial_num = trial + 1

                    if args.agent in ["generator", "both"]:
                        generator_result = await run_generator_agent(
                            gc, cg, dst_func, args.sanitizer, trial_num, args.trials
                        )
                        if generator_result:
                            cg_key = f"{harness_name}_{cg.name}_trial_{trial_num}"
                            all_generator_results[cg_key] = generator_result

                    if args.agent in ["blobgen", "both"]:
                        blobgen_result = await run_blobgen_agent(
                            gc, cg, dst_func, args.sanitizer, trial_num, args.trials
                        )
                        if blobgen_result:
                            cg_key = f"{harness_name}_{cg.name}_trial_{trial_num}"
                            all_blobgen_results[cg_key] = blobgen_result

            # Print summary
            logger.info("=" * 50)
            logger.info("SUMMARY")
            logger.info("=" * 50)

            if args.agent in ["generator", "both"]:
                logger.info(
                    f"GeneratorAgent results: {len(all_generator_results)} CGs with"
                    " crashes"
                )
                total_crashes = sum(
                    len(crashes) for crashes in all_generator_results.values()
                )
                logger.info(f"Total crashes found: {total_crashes}")

            if args.agent in ["blobgen", "both"]:
                logger.info(
                    f"BlobGenAgent results: {len(all_blobgen_results)} CGs with"
                    " payloads"
                )
                total_payloads = sum(
                    len(payloads) for payloads in all_blobgen_results.values()
                )
                logger.info(f"Total payloads generated: {total_payloads}")


if __name__ == "__main__":
    asyncio.run(main())
